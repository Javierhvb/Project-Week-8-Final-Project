{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta, date\n",
    "from textblob import TextBlob\n",
    "from twitterscraper import query_tweets\n",
    "import datetime as dt\n",
    "import requests\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "stopwords = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name of the columns from the received dataframe from the twitterscrapper\n",
    "cols =['screen_name', 'username', 'user_id', 'tweet_id', 'tweet_url',\n",
    "       'timestamp', 'timestamp_epochs', 'text', 'text_html', 'links',\n",
    "       'hashtags', 'has_media', 'img_urls', 'video_url', 'likes', 'retweets',\n",
    "       'replies', 'is_replied', 'is_reply_to', 'parent_tweet_id',\n",
    "       'reply_to_users']\n",
    "companies = ['amazon','nike', 'netflix', 'tesla', 'starbucks', 'yelp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collecting the tweets from a keyword, in a range of days,collecting at least 1.000, in english\n",
    "def tweets_dataframe(company,begin_date, end_date, limit = 1000, lang='en'):\n",
    "    total_tweets = query_tweets(company,begindate = begin_date, enddate = end_date, limit = limit, lang = lang)\n",
    "    df = pd.DataFrame(t.__dict__ for t in total_tweets)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#support function to iterate through a date range\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collecting daily tweets in a range of time from a company. Combining the tweets dataframe function with the daterange to have them daily.\n",
    "#the csv is saved by appending the new tweets at the end of the csv. Therefore we can stop the kernel at anytime without losing any data.\n",
    "#The appending its done because the function takes hours.\n",
    "\n",
    "def appending_tweets(company, begin_date, end_date):\n",
    "    tweet_cols = pd.DataFrame(columns = cols).to_csv(f'{company}_tweets.csv')\n",
    "    for single_date in daterange(begin_date, end_date):\n",
    "        tweets = tweets_dataframe(company, single_date, single_date + timedelta(1))\n",
    "        tweets.to_csv(f'../02 CSV_files/trends/tweets/{company}_tweets.csv', mode='a', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Applying the appending_tweets function to a list of companies, with a beging date and an end date\n",
    "def gathering_companies_tweets(companies_list, begin_year, begin_month, begin_day, end_year, end_month, end_day):\n",
    "    begin_date = dt.date(begin_year, begin_month, begin_day)\n",
    "    end_date = dt.date( end_year, end_month, end_day+ 1)\n",
    "    for company in companies_list:\n",
    "        appending_tweets(company, begin_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gathering_companies_tweets(companies, 2019,1,1,2020,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the data from the tweets for doing a Sentiment analysis on them\n",
    "#only keeping letters and taking out webpages\n",
    "def clean_up(s):\n",
    "     return re.sub(\"http\\S+|[^a-zA-Z]\", \" \", s.lower())\n",
    "\n",
    "#tokenizing the data\n",
    "def tokenize(s):\n",
    "    return nltk.word_tokenize(s)\n",
    "\n",
    "#stemming and lemmatizing the data\n",
    "def stem_and_lemmatize(l):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    return [lemmatizer.lemmatize(stemmer.stem(word)) for word in l]\n",
    "\n",
    "#removing the stopwords\n",
    "def remove_stopwords(l):\n",
    "    return [word for word in l if word not in stopwords]\n",
    "\n",
    "#applying all the previous functions to a text\n",
    "def bag_of_words(string):\n",
    "    string = clean_up(string)\n",
    "    string = tokenize(string)\n",
    "    string = stem_and_lemmatize(string)\n",
    "    string = remove_stopwords(string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get the subjectivity and polarity of a text. The score is to say if its positive or negative\n",
    "def get_subjectivity(tweet):\n",
    "    return TextBlob(str(tweet)).sentiment.subjectivity\n",
    "def get_polarity(tweet):\n",
    "    return TextBlob(str(tweet)).sentiment.polarity\n",
    "def get_analysis(score):\n",
    "    if score < 0:\n",
    "          return 'Negative'\n",
    "    elif score == 0:\n",
    "          return 'Neutral'\n",
    "    else:\n",
    "          return 'Positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the previously defined functions to analysis the tweets analysis. Cleaning the text and given a score to each tweet\n",
    "# tweet relevance score is to weigth each tweets sentiment according to the number of likes and tweets it has.\n",
    "def get_tweets_analysis(df):\n",
    "    df['processed_text'] = df['text'].apply(bag_of_words)\n",
    "    print('bag of words: done')\n",
    "    df['subjectivity'] = df['processed_text'].apply(get_subjectivity)\n",
    "    print('subjectivity: done')\n",
    "    df['polarity'] = df['processed_text'].apply(get_polarity)\n",
    "    print('polarity: done')\n",
    "    print('starting analysis')\n",
    "    df['analysis'] = df['polarity'].apply(get_analysis)\n",
    "    print('finished analysis...')\n",
    "    df['tweet_relevance'] = (df['polarity']*(1+df['likes'])*(1+df['retweets']))\n",
    "    print('last effort')\n",
    "    df = df[['timestamp','likes', 'retweets', 'subjectivity','polarity','analysis', 'tweet_relevance','company']]\n",
    "    print('enjoy!')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the previously gathered tweets, cleaning & analysing them and saving all of them into one csv file.\n",
    "def analysing_tweets(companies_list):\n",
    "    for company in companies_list:\n",
    "        total_tweets = pd.DataFrame(columns =['timestamp','likes', 'retweets', 'subjectivity','polarity','analysis', 'tweet_relevance'])\n",
    "        df_company = pd.read_csv(f'../02 CSV_files/tweets/{company}_tweets.csv')\n",
    "        company = get_tweets_analysis(df_company)\n",
    "        company['name'] = company\n",
    "        total_tweets = total_tweets.append(company)\n",
    "        total_tweets.drop('isPartial', axis = 1, inplace = True)\n",
    "        total_tweets.to_csv(f'../02 CSV_files/tweets/cleaned_tweets.csv', mode='a', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def companies_trends_peaks(companies_list):\n",
    "    hourly_trends = pd.read_csv('../02 CSV_files/trends/hourly_peaks.csv')\n",
    "    companies_df = {}\n",
    "    trends_df = pd.DataFrame()\n",
    "    for company in companies_list:\n",
    "        #collecting each companies all time peaks in google trends (gathered in the company trends jupyter notebook)\n",
    "        #keeping only the date and the trend value\n",
    "        companies_df[company] = hourly_trends[['date', company]]\n",
    "        companies_df[company] = companies_df[company].dropna()\n",
    "        #creating and extra column with the companies name\n",
    "        companies_df[company]['name'] = company\n",
    "        #renaming the columns with the trends value (which was the company name) to trends\n",
    "        companies_df[company] = companies_df[company].rename(columns = {company:'trends'})\n",
    "        #appending the company peaks to the complete dataframe\n",
    "        trends_df = trends_df.append(companies_df[company])\n",
    "    #converting the columsn to datatypes\n",
    "    trends_df['date'] = pd.to_datetime(trends_df['date']).dt.date\n",
    "    #grouping by company and date because the peaks implied the week were the peak was achieved and the previous one\n",
    "    trends_df = trends_df.groupby(['date','name']).agg({'trends' : 'mean'}).reset_index()\n",
    "    #returning the dataframe with the companies highest peaks per day\n",
    "    return trends_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_peaks = companies_trends_peaks(companies)\n",
    "#iterating through the companies peaks dataframe in order to gather each of the days peaks tweets to see the sentiment analysis.\n",
    "#the appending_tweets function already saves them in their companies csv file\n",
    "def collecting_tweets_dataframe(df):\n",
    "    for index, row in df.iterrows():\n",
    "        appending_tweets(row['name'], row['date'], row['date'] + timedelta(days = 1))\n",
    "\n",
    "collecting_tweets_dataframe(companies_peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading all the saved tweets and unifiying them into one dataframe to rule them all. Saving it as gathered tweets and returning it in case you want to use it\n",
    "def creating_unified_df(companies_list):\n",
    "    tweets = pd.DataFrame()\n",
    "    for company in companies_list:\n",
    "        new_company = pd.read_csv(f'../02 CSV_files/tweets/{company}_tweets.csv', index_col = 0)\n",
    "        new_company['company'] = company\n",
    "        tweets = tweets.append(new_company)\n",
    "    tweets.to_csv('../02 CSV_files/tweets/gathered_tweets.csv')\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = creating_unified_df(companies)\n",
    "analysed_tweets = get_tweets_analysis(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysed_tweets.to_csv('02 CSV_files/tweets/analysed_tweets_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the final tweets dataframe, after gathering and analysing all the tweets\n",
    "def prepare_final_tweets_df(file_name):\n",
    "    #reading the file from the tweets file\n",
    "    tweets = pd.read_csv(f'../02 CSV_files/tweets/{file_name}.csv', index_col = 0)\n",
    "    #changing the name to be consistent with the other gathered datasets\n",
    "    tweets.rename(columns = {'timestamp':'date', 'company':'name'}, inplace = True)\n",
    "    #taking out the neutral tweets\n",
    "    tweets = tweets.loc[lambda x: x['analysis'] != 'Neutral']\n",
    "    #converting the date column into date type\n",
    "    tweets.date = pd.to_datetime(tweets.date).dt.date\n",
    "    #assigning a score to the tweet. 1 if its positive, -1 if its negative\n",
    "    tweets['score'] = tweets['polarity'].apply(lambda x : 1 if x > 0 else -1)\n",
    "    #keeping only 3 columns. Date, company and the score\n",
    "    tweets = tweets[['date', 'name', 'score']]\n",
    "    #groping by date and company, having a mean of the score columns. Therefore, we have for each date thousands of analysed tweets about each company every day\n",
    "    tweets_df = tweets.groupby(['date', 'name']).agg({'score':'mean'}).reset_index()\n",
    "    return tweets_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prepared_tweets = prepare_final_tweets_df('analysed_tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating columns referencing previous rows, in order to see the evolution of the score\n",
    "def shift_n_rows(df,col_name,n_rows):\n",
    "    for col in range(1,n_rows+1):\n",
    "        df[f'{col_name}_day_{col}'] = df[col_name].shift(col)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the shift function individualy to each company and combining it afterwards into one dataframe\n",
    "def create_shifted_df(companies_list, df ,col_name, n_of_shifts):\n",
    "    shift = {}\n",
    "    shifted = pd.DataFrame()\n",
    "    for company in companies:\n",
    "        new_df = df[df['name'] == company]\n",
    "        new_df = shift_n_rows(new_df, col_name, n_of_shifts)\n",
    "        shift[company] = new_df\n",
    "        shifted = shifted.append(new_df)\n",
    "    shifted.dropna(axis = 0, inplace = True)\n",
    "    shifted.date = pd.to_datetime(shifted.date)\n",
    "    shifted.to_csv('../02 CSV_files/csv_finals/final_tweets.csv')\n",
    "    return shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_shifted = create_shifted_df(companies, prepared_tweets, 'score', 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_shifted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
